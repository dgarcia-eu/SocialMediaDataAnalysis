{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1004d8b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\" >\n",
    "<h1 style=\"margin-top: 0.2em; margin-bottom: 0.1em;\">Assignment 4</h1>\n",
    "<h4 style=\"margin-top: 0.7em; margin-bottom: 0.3em; font-style:italic\">Commit your solutions to GitHub until July 23, 23:59</h4>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01196ea9-5dff-4279-b0a7-d9ea286e0c19",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "## Social Network Analysis of Swiss Politicians on Twitter Data\n",
    "In the first part of this assignment you will do the following tasks:\n",
    "1. Build social network of retweets\n",
    "2. Calculate assortativity\n",
    "3. Permutation tests\n",
    "4. Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aac7b3",
   "metadata": {},
   "source": [
    "### Install requirements. \n",
    "\n",
    "The following cell contains all the necessary dependencies needed for this task. If you run the cell everything will be installed.  \n",
    "\n",
    "* [`pandas`](https://pandas.pydata.org/docs/index.html) is a Python package for creating and working with tabular data. [Here](https://pandas.pydata.org/docs/reference/index.html) is the documentation of `pandas`.\n",
    "* [`numpy`](https://numpy.org/) is a Python package for mathematical functions. [Here](https://numpy.org/doc/stable/reference/index.html) is the documentation of `numpy`.\n",
    "* [`matplotlib`](https://matplotlib.org/) is a Python package for creating plots. [Here](https://matplotlib.org/stable/api/index.html) is the documentation of `matplotlib`.\n",
    "* [`networkx`](https://networkx.org/) is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. [Here](https://networkx.org/documentation/stable/reference/index.html) is the documentation of `networkx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234eb790-6def-45a2-b436-406eb1be62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install numpy\n",
    "! pip install matplotlib\n",
    "! pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb2e59",
   "metadata": {},
   "source": [
    "### Import requirements\n",
    "The cell below imports all necessary dependancies. Make sure they are installed (see cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e909d35-ff4c-45fc-8642-c2be8d3850c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c2bf5-29c6-451e-af54-893849528056",
   "metadata": {},
   "source": [
    "### Exercise 1: Load social networks of retweets *(1 point)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa340f4",
   "metadata": {},
   "source": [
    "The attached `swiss_pol_retweet_network.gexf` file contains an undirected retweet network of Swiss politicians for the time between 2021-07-12 and 2022-07-12. Each node in the network is a represents a politician, and stores their Twitter user id, username, and party affiliation. An edge exists between a pair of politicians that exchanged at least one retweet with each other (regardless of the direction).\n",
    "\n",
    "* How many nodes and edges are there in the network?\n",
    "* Visualize the graph. Use [`draw_networkx`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d66bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f9f1b8-1b55-43b5-bc37-7389716008d5",
   "metadata": {},
   "source": [
    "### Exercise 2: Calculate graph assortativity *(2 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ad210-6321-4625-9732-7da2c639da9d",
   "metadata": {},
   "source": [
    "Use the function [`attribute_assortativity_coefficient`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.assortativity.attribute_assortativity_coefficient.html) of `networkx` to calculate the assortativity with respect to party labels. How high is the value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed1868-86cc-4752-b88a-b8a15e6cdfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fbc8610-98e4-44e3-8774-ccf7d4e54922",
   "metadata": {},
   "source": [
    "To see if the assortativity value fits your expectations, use the function [`draw_networkx`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html) to plot the network coloring each node according to the political party label of the politician. Does the pattern of colors fit the value of assortativity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd73c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "399248d1-6cdc-46df-a15e-93c54393ae28",
   "metadata": {},
   "source": [
    "### Exercise 3: Permutation tests *(2 points)*\n",
    "\n",
    "Next, we are going to use a permutation test to test whether the above result could have happened at random. \n",
    "\n",
    "First, let's run a permutation. Perform the same assortativity calculation as above but permuting the party labels of nodes. \n",
    "\n",
    "Also set the party for each node as node attribute by using [`set_node_attribute`](https://networkx.org/documentation/stable/reference/generated/networkx.classes.function.set_node_attributes.html) (be carefull the parties are now permuted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c701aa1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "349eda64-ccf7-4c28-881b-3021fbafad2c",
   "metadata": {},
   "source": [
    "Is the value much closer to zero?\n",
    "Repeat the calculation with 1000 permutations and plot the histogram of the resulting values. Add a line with the value of the assortativity without permutation. Is it far or close to the permuted values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ec28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a9ec0ff-8744-4738-a95e-5d787b71822a",
   "metadata": {},
   "source": [
    "To be sure, let's calculate a p-value for the null hypothesis that the assortativity is zero and the alternative hypothesis that it is positive (what we expected):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4c9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5c4e406-dd0a-4c3a-a47b-9c92fa3686d0",
   "metadata": {},
   "source": [
    "After looking at the above results, do you think it is likely that the assortativity we found in the data was produced by chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa8bf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d858242-9686-4a29-8b7e-7b50fdbea324",
   "metadata": {},
   "source": [
    "### Exercise 4: Community detection *(3 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91874d7b-7a87-4b78-bb88-02c4fd3e62cc",
   "metadata": {},
   "source": [
    "Let's test if Twitter communities match political affiliations. Remove nodes with degree zero in the network and run the [Louvain community detection algorithm](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html). Visualize the result coloring nodes by community labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5c825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4ed190-ff56-4210-a8ce-e366fa418ba4",
   "metadata": {},
   "source": [
    "Run the [`modularity`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html) function with the above community labels. Is it high enough to think that the network has a community structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba26b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeec818d-32f9-4c6f-be91-f475f3127da7",
   "metadata": {},
   "source": [
    "Repeat but using the party labels instead of the communities detected with Louvain. Is it higher or lower? How far is this modularity from the maximal one found with Louvain?\n",
    "\n",
    "For this iterate over the parties and filter a subset of users that is in the given party and in the graph. Add the ids of these partymembers (do not include any duplicates) and repeat this for all parties.\n",
    "\n",
    "Afterwards you can calculate the modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f485cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462a4aa1-0b8e-4c01-bb60-aaad998eabac",
   "metadata": {},
   "source": [
    "Finally, to understand which parties are represented in each community, build a data frame for nodes with two columns: one with the party label and another one with the community label. Use the [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) function to print a contingency table. Which party or parties compose each community?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cdb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "495e2320-8af1-4a72-b50e-66ce0efdbce6",
   "metadata": {},
   "source": [
    "### Exercise 5: Prediction and discussion of other methods *(3 points)*\n",
    "* How well can you predict the party of a politician from its neighbors in the network? Here you can use the rule of predicting the party as the majority party among its neighbors and evaluate the accuracy of this approach.\n",
    "* What would be the results if we use the network of replies? Do you expect assortativity and modularity to be higher or lower?\n",
    "* If you retrieved data of follower links, you can repeat the above analysis for undirected following relationships. Do you expect a higher or lower assortativity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0b333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "270c42f4",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "## Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ebddb",
   "metadata": {},
   "source": [
    "### Exercise 6: Data Collection *(3 points)*\n",
    "\n",
    "#### Sign up for the Reddit API\n",
    "* In this part of the assignment we will collect data using the Reddit API, and compare the tree structure of political and non-political subreddits.\n",
    "* First, you need to sign up for the Reddit API. For this, follow the steps outlined in [this guide](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c). You will need to create an app on the following [link](https://old.reddit.com/prefs/apps/).\n",
    "* Next, install the [PRAW package](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html), which provides a nice wrapper for the Reddit API.\n",
    "\n",
    "#### Collect the data\n",
    "* Navigate to the following [link](https://www.reddit.com/best/communities/1/) and select 4 political, and 4 non-political subreddits. Ideally, you would want subreddits with around 100-200 thousand members. You should select subreddits with enough engagement, but ones which do not typically have a thousand replies to each submission, since the API has a relatively low rate limit.\n",
    "* Extract the top 20 `hottest` submissions from each of your selected subreddits, ignoring `pinned` submissions.\n",
    "* For each of the submissions, extract all the comments and replies, and store them, so that you don't need to rerun this step later. Make sure to save the `id`, of the post, the id of its `parent` (the post that it replies to) and the name of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86255e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e619432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd9305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08ff719c",
   "metadata": {},
   "source": [
    "### Exercise 7: Analysis *(3 points)*\n",
    "* Create a network/tree for each of the submissions, for this, you may use the [networkx](https://networkx.org/documentation/stable/tutorial.html) package, or create your own classes to store the data.\n",
    "* For each of the trees, calculate the `maximum depth` and `maximum width`. By maximum depth, we mean the number of edges between the root node, and the furthest leaf node (i.e. the reply which is deepest in the comment tree). The maximum width of the tree is the maximum number of comments, replies on one \"level\". On the first \"level\" is the submission itself, on the next one the comments replying directly to the submission, on the third are the comments replying to the comments on the first level, and so on.\n",
    "* Also calculate the `number of nodes` for each of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551cfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5a6c97d",
   "metadata": {},
   "source": [
    "### Exercise 8: Comparison and interpretation *(3 points)*\n",
    "* Compare the mean number of nodes, mean maximum width, and mean maximum depth of political and non-political subreddits. What differences can you notice?\n",
    "* Can you conduct a statistical test to see if the differences are significant? (conduct the test if you found one which is feasible)\n",
    "* Compare the distribution of the maximum width and maximum height of political vs non political subreddits by plotting their relative frequencies.\n",
    "* Create a scatterplot with the log of max width of the tree on the x-axis, and the max depth of the tree on the y-axis. Color the dots based on their group (political vs. non-political). Add a large dot for both groups to show the mean of the group. \n",
    "* Interpret your results.\n",
    "* What are the limitations of this analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d03c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "46e2835a142a16ae115bce5fddf19f27ce13b17a4ab8ded638c88ab5ce5171d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
